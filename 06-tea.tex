% \ej{Remove "this paper"; Rephrase the language describing code and data releases.}
\begin{flushright}
    \begin{quote}
        \textit{
        The enormous variety of modern quantitative methods leaves researchers with the
        nontrivial task of matching analysis and design to the research question.} \\ 
        \vspace{-10pt}
        \begin{flushright}
            \textit{                                 - Ronald Fisher~\cite{fisher1937design}}
        \end{flushright}
    \end{quote}
\end{flushright}
    
Since the development of modern statistical methods (e.g., Student's t-test,
ANOVA, etc.), statisticians have acknowledged the difficulty of identifying
which statistical tests people should use to answer their specific research
questions. Almost a century later, choosing appropriate statistical tests for
evaluating a hypothesis remains a challenge. As a consequence, errors in
statistical analyses are common~\cite{kaptein2012rethinking}, especially given
that data analysis has become a common task for people with little to no
statistical expertise.

A wide variety of tools (such as SPSS~\cite{wiki:spss}, SAS~\cite{wiki:sas}, and
JMP~\cite{wiki:jmp}), programming languages (e.g., R~\cite{wiki:r-language}),
and libraries (including numpy~\cite{oliphant2006numpy}, scipy~\cite{scipy}, and
statsmodels~\cite{statsmodelsPaper}), enable people to perform specific
statistical tests, but they do not address the fundamental problem that users
may not know which statistical test to perform and how to verify that specific
assumptions about their data hold. In fact, all of these tools place the burden
of valid, replicable statistical analyses on the user and demand deep knowledge
of statistics.

Users not only have to identify their research questions, hypotheses, and domain
assumptions, but also must select statistical tests for their hypotheses (e.g.,
Student's t-test or one-way ANOVA). For each statistical test, users must be
aware of the statistical assumptions each test makes about the data (e.g.,
normality or equal variance between groups) and how to check for them, which
requires additional statistical tests (e.g., Levene's test for equal variance),
which themselves may demand further assumptions about the data. This cognitively
demanding process requires significant knowledge about statistical tests
and their preconditions as well as the ability to perform the tests and verify their
preconditions. This process can easily lead to mistakes.

In response, we design and developed Tea\footnote{named after Fisher's ``Lady
Tasting Tea'' experiment~\cite{fisher1937design}}, a high-level declarative
language for automating statistical test selection and execution that abstracts
the details of statistical analysis from the users. Tea captures users'
hypotheses and domain knowledge, translates this information into a constraint
satisfaction problem, identifies all valid statistical tests to evaluate a
hypothesis, and executes the tests. Tea's higher-level, declarative nature aims
to lower the barrier to valid, replicable analyses.

Tea is easy to integrate directly into common data analysis workflows for users
who have minimal programming experience. Tea is implemented as an open-source
Python library, so programmers can use Tea wherever they use Python, including
within Python notebooks.

In addition, Tea is flexible. Its abstraction of the analysis process and use of
a constraint solver to select tests is designed to support its extension to
emerging statistical methods, such as Bayesian analysis. Currently, Tea supports
frequentist Null Hypothesis Significance Testing (NHST).

This work makes the following contributions:
\begin{itemize}
    \item a novel DSL for automatically selecting and executing statistical
    analyses based on users' hypotheses and domain knowledge
    (\autoref{sec:TeaPL}), 
    \item a runtime system that formulates statistical test selection as a maximum constraint satisfaction problem (\autoref{sec:TeaRS}), and
    \item an initial evaluation showing that Tea can express and execute common NHST statistical tests (\autoref{sec:evalTea}). 
\end{itemize}

After describing related work, the chapter describes a usage scenario, providing
an overview of Tea (\autoref{usageScenarioTea}). Then, we discuss the concerns
about statistics in the HCI community that shaped Tea's
design~(\autoref{sec:design}), the implementation of
\TeaPL~(\autoref{sec:TeaPL}), the implementation of
\TeaRS~(\autoref{sec:TeaRS}), and the evaluation of Tea as a
whole~(\autoref{sec:evalTea}). The chapter concludes with a discussion of Tea's
goals, limitations, and future work (\autoref{sec:discussionTea}) and a summary
of how \tea demonstrates my thesis(\autoref{sec:summaryTea})

% As we found in our analysis of tools (\autoref{sec:toolsAnalysis}), a wide
% variety of tools (such as SPSS~\cite{wiki:spss}, SAS~\cite{wiki:sas}, and
% JMP~\cite{wiki:jmp}), programming languages (e.g., R~\cite{wiki:r-language}),
% and libraries (including numpy~\cite{oliphant2006numpy}, scipy~\cite{scipy}, and
% statsmodels~\cite{statsmodelsPaper}), enable people to perform specific
% statistical tests, but they do not address the fundamental problem that users
% may not know which statistical test to perform and how to verify that specific
% assumptions about their data hold. 

% To address this overlooked need, we designed Tea\footnote{named after Fisher's
% ``Lady Tasting Tea'' experiment~\cite{fisher1937design}}, a high-level
% declarative language for automating statistical test selection and execution
% that abstracts the details of statistical analysis from the users. Tea captures
% users' hypotheses and domain knowledge (\higherLevel, \connectConceptualStats),
% reformulates these into a constraint satisfaction problem, identifies all valid
% statistical tests to evaluate a hypothesis, and executes the tests. 

\input{tea/related_work.tex}
\vspace*{-0.5cm}
\input{tea/usage_scenario.tex}

\input{tea/design_considerations.tex}

\input{tea/system.tex}

\input{tea/eval.tex}

\section{Discussion, Limitations, and Future Work} \label{sec:discussionTea} 

Our goal with Tea was to determine the feasibility of automating statistical
test selection based on high-level input from analysts. Automating statistical
test selection raises important concerns about the impact of such automation on
the reliability of statistical conclusions. In this regard, there are two chief
concerns pertaining to (i) selective inference and (ii) multiple testing, both
of which inflate the Type I Error Rate and can lead to more false discoveries. 

Tea relies on statistical tests (e.g., Shapiro-Wilk's test for normality) to
assess properties of data to determine which statistical tests (e.g., Student's
t-test) are used to assess the input hypothesis. Repeated property testing of
the data is a form of ``double-dipping''~\cite{}, or using the data to make
decisions about analyses on the data. Preventing this would be ideal to reduce
the false positive discovery rate. However, the statistics community is still
developing techniques to address this issue. A naive approach would be to only
use a sample of the data to determine the final statistical test and then use
another sample to make statistical inferences. While viable for large datasets,
this may not be possible for smaller datasets. A more recently proposed
technique, data fission~\cite{} overcomes, in theory, this dependence on dataset
size.  Data fission introduces noise to the data to make analysis decisions
(i.e., statistical test selection) and then stripping the noise to obtain final
results. Tea does not currently implement either of these approaches. In the
future, Tea should incorporate these and future recommendations from the
statistics community. 

Furthermore, there is an inherent tension between executing multiple statistical
tests (e.g., Student's t-test and Welch's t-test) to show analysts the
robustness, or sensitivity, of statistical results and increasing the number of
comparisons performed. In Tea, we believed that providing analysts with the
ability to compare statistical tests, make sensitivity judgments, and report the
results of a test most common in their disciplines was more important than
restricting the number of statistical tests, especially we have observed
analysts intentionally run multiple statistical tests in order to compare
results on their own. To more fully support sensitivity analyses and discourage
cherry-picking statistical tests and results, Tea should provide more explicit
support for interpreting, comparing, and contrasting statistical test results in
the future. This will be particularly important in scenarios where statistical
tests may disagree with one another. conflicting test conclusions. 

% Future work should take
% advantage of the multiple statistical test outputs as a check on the robustness
% of conclusions and help analysts make sense of the results holistically.
%This is well-aligned with the intention of multiverse analyses to improve robust data analysis. 

% This could mitigate concerns that Tea encourages p-hacking and cherry-picking
% results that are convenient for the analyst, given that Tea may output multiple
% possible statistical tests. 

% The output of executing a Tea program with data is one or more
% statistical test results. 

Finally, Tea's test selection is well suited for answering a class of relatively simple
research questions. At the same time, there are more complex research questions
that analysts want to ask about their domain using data that require more
complex statistical analyses. These are currently out of reach for \tea. For
instance, domain experts may not want to know that there is a difference between
treatment and control groups but also estimate the influence of the treatment on
an outcome in the presence of other variables that also influence treatment and
the outcome. Therefore, in order to support a larger class of research questions
and statistical models, we need to re-consider and extent Tea's abstractions and
constraint-based reasoning approach. 

\begin{comment}
Lessons learned

Why constraints? are they really necessary?

- inflated alpha
- inherent tension in executing multiple statistical tests vs. sensitivity


Multiple testing

Design 

Validity 
\end{comment}


\begin{comment}
To further prevent cherry picking and more holistic understanding of the robustness of conclusion

Reasoning: Constraints were well-suited for statistical test selection, for
which there is general consensus about the applicability of tests. However, that
is not always the case....

Interpretation of results -- tension between 

To further prevent cherry picking and more holistic understanding of the robustness of conclusions

At the time of Tea's release (circa late 2019), we believed that \tea's
abstraction and modularity would enable the incorporation of other statistical
analysis approaches, such as Bayesian inference, as they move into the
mainstream. Although we have yet to incorporate Bayesian inference into \tea, or
more generally provide tool support for authoring Bayesian analyses, our
experience generalizing \tea's approach to statistical modeling in \tisane has
shown us the importance of making even more explicit implicit connections
between variables (abstraction) and providing reasoning approaches that match
analysis needs.


Testing to estimation 

NHST to statistical modeling

How do we support this larger and more complex class of use cases? Can we
generalize our approach in Tea--making implicit assumptions explicit and
automatically reasoning about these assumptions to identify valid analyses--to
statistical models? To answer this question, we set out to develop a holistic
understanding of how analysts translate research questions into statistical
analyses. 

\subsection{Ongoing development}
With teams of undergrads, I have continued to improve Tea in two specific ways. 

First, recent development has focused on updating the outputs of Tea to include
(i) interactive visualizations and (ii) text for reporting the statistical
results in the American Psychological Association's recommended formats for each
valid statistical test. The interactive visualizations aim to illustrate what
the results of the statistical tests mean, such as scatterplots for correlations
and heatmaps for the Chi-squared test. We selected the visualizations for each
test based on recommendations from Franconeri et
al.~\cite{franconeriVisualizationChooser}, what existing tools such as
JMP~\cite{jmp} already use, and our own experiences using and trying to
communicate statistical results. Development and initial user testing is on
track to wrap up by the end of spring quarter. 

Second, a usability issue with Tea's current API is its reliance of ``magic
strings.'' We are currently refactoring the API to be more object-oriented by
extending Tisane's variables data classes. We hope this revision will be more
usable with ``free'' help from existing IDEs such as VSCode that provide API
suggestions inline when specifying parameters. 

Both features will be incorporated into a new release of Tea, which I have
currently scheduled for June, 2022. 
\end{comment}

\section{Summary of Contributions} \label{sec:summaryTea}

% Reminder: Thesis statement
% Domain-specific languages that provide abstractions for expressing conceptual
% knowledge, data collection procedures, and analysis intents instead of specific
% statistical modeling decisions coupled with automated reasoning to compile
% conceptual specifications into statistical analysis code help statistical
% non-experts more readily author valid analyses. 

% A common approach to assessing support for conceptual hypotheses in data is to
% use statistical tests (e.g., Student's t-test, Chi-Square test, ANOVA).

Statistical tests (e.g., Student's t-test, Chi-Square test, ANOVA) are a common
approach for assessing conceptual hypotheses with data.
Statistical testing requires analysts to grapple with their conceptual
hypotheses, know a number of tests and when they are applicable (i.e., know the
preconditions for when these tests hold), assess the applicability of tests
(i.e., check preconditions), and pick and implement specific tests using
low-level APIs. 

Tea's key insight is that we can reformulate statistical test
selection as a constraint satisfaction problem. We designed and implemented a
higher-level DSL around this insight that takes an analyst's hypothesis and
assumptions about their data as input and provides the results of executing
valid statistical tests as output. In an evaluation, we found that Tea avoids
faulty test selection and conclusions that are easy to make using existing
tools. In this way, \tea improves statistical conclusion and internal validity~\cite{shadish2010campbell}. 

Tea demonstrates the feasibility and benefit of developing systems that
emphasize \textit{higher-level abstractions} (\thesisChallengeExplicit) and
\textit{automated reasoning} (\thesisChallengeRep) for statistical tests.
However, using statistics to answer real-world questions requires going beyond
statistical testing to grappling with statistical modeling and effect
estimation. Next, we consider how our approach generalizes to a larger class of
statistical analyses. 

\begin{comment}
% Replace the conclusion section with a summary section. Again, you should tie this chapter back to the main themes of the thesis.
4-5 sentences: 
1. Restate problem 
2. Articulate core contributions: problem/idea + technical
3. Key Evaluation results
4. 1 soundbite/takeaway
5. Transition to next chapter. 

1. Restate problem 

A common approach to assessing support for conceptual hypotheses in data is to
use statistical tests (e.g., Student's t-test, Chi-Square test, ANOVA).
Statistical testing requires analysts to grapple with their conceptual
hypotheses, know a number of tests and when they are applicable (i.e., know the
preconditions for when these tests hold), assess the applicability of tests
(i.e., check preconditions), and pick and implement specific tests using
low-level APIs. 

2. Articulate core contributions: problem/idea + technical

Tea's key insight is that we can reformulate statistical test selection as a
constraint satisfaction problem. We designed and implemented a higher-level DSL
around this insight that takes an analyst's hypothesis and assumptions about
their data as input and provides the results of executing valid statistical
tests as output. 

3. Key Evaluation results

In an evaluation, we found that Tea avoids faulty test
selection and conclusions that are easy to make using existing tools.

4. 1 soundbite/takeaway 

It is possible to design higher level language and automated reasoning to select
valid statistical tests that are widely used and even avoid faulty conclusions
that are easy to make using existing tools. The key is to make implicit
assumptions about the data explicit and reason about analyst assumptions and
computed data properties together in a logical constraint system. 

5. Transition to next chapter. 

However, an important limitation to overcome in the future chapters, most
notably in Tisane (~\autoref{chapter:tisane}) is how to generalize automated
support for more complex research questions and statistical analyses. 

Tea demonstrates the feasibility and benefit of developing systems that
emphasize \textit{higher-level abstractions} and \textit{automated reasoning}
for statistical tests (\autoref{para:thesisStatement}). Next, we consider how
this approach generalizes to a larger class of statistical analyses. could
\end{comment}

\textit{This work was done in collaboration with Maureen Daum, Jared Roesch, Sarah E.
Chasins, Emery Berger, \reneJust, and Katharina Reinecke. It was originally
published and presented at \uistConf{2019}~\cite{jun2019tea}. Since publication, multiple
people, including most notably Shreyash Nigam, Reiden Chea, and Annie Denton,
have contributed to updating and improving Tea.}
