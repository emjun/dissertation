If you are copying and pasting material from one of your papers, then remember to:
\begin{itemize}
    % \item Remove the abstract and instead add a little overview of the chapter and how it ties in to the rest of the thesis. You should also mention the original paper's source like: ``This chapter includes materials originally published in $\backslash$citet\{myownppr\}''
    % \item Make sure the formatting still works -- this is single column now!
    \item Consider rephrasing conference-paper-style language:
    \begin{itemize}
        \item Find every place you mention some variation of ``in this paper'' and say ``in this chapter'' instead.
        \item Remove or rephrase the parts where you talk about ``our main contributions''.
        \item Rephrase the language describing code and data releases.
    \end{itemize}
    % \item Replace the conclusion section with a summary section. Again, you should tie this chapter back to the main themes of the thesis.
\end{itemize}

{\color{orange} Write an overview of chapter, how it ties to the rest of the thesis.}

Although prior work has observed \textit{that} data analysis is
iterative~\cite{liu2019paths,grolemund2014cognitive} and involves multiple
levels of considerations~\cite{liu2019understanding}, \textit{how} analysts move
between these cognitive, statistical, and computational realities iteratively
has remained under-scrutinized. To fill this gap, we conducted a content analysis of 50 published
empirical publications from diverse disciplines and a lab study with 24 data
scientists authoring analyses to identify the steps and challenges involved in
authoring analyses. 

Based on our content analysis and lab study findings, we coin and define
\textit{hypothesis formalization} as a dual-search process~\cite{klahr1988dual}
that involves developing and integrating cognitive representations from two
different perspectives---conceptual hypotheses and concrete model
implementations. Analysts move back and forth between these two perspectives
during formalization while balancing conceptual, data-driven, statistical, and
programming implementation constraints. Analysts iterate over conceptual steps
to refine their hypothesis in a \textit{hypothesis refinement loop}. Analysts
also iterate over computational and implementation steps in a \textit{model
implementation loop}. Data collection and data properties may also prompt
conceptual revisions and influence statistical model implementation.

\input{hypothesisFormalization/related_work.tex}

\input{hypothesisFormalization/content_analysis.tex}

\input{hypothesisFormalization/lab_study.tex}

\input{hypothesisFormalization/tools_analysis.tex}

\input{hypothesisFormalization/implications.tex}

\input{hypothesisFormalization/discussion.tex}

\section{Summary of Contributions}
% Replace the conclusion section with a summary section. Again, you should tie this chapter back to the main themes of the thesis.

% Reminder: Thesis statement
% Domain-specific languages that provide abstractions for expressing conceptual
% knowledge, data collection procedures, and analysis intents instead of specific
% statistical modeling decisions coupled with automated reasoning to compile
% conceptual specifications into statistical analysis code help statistical
% non-experts more readily author valid analyses. 

The empirical studies that led us to articulate the theory of hypothesis
formalization illustrates the key challenge to authoring data analyses: Analysts
must translate their implicit domain knowledge into statistical specifications
that they can implement and execute in code. As we saw in the lab study,
analysts often resort to changing their hypotheses or research questions to what
they can implement or get stuck on how to represent their conceptual knowledge
in statistical models, highlighting the dual-search nature of hypothesis
formalization. Furthermore, the summary of hypothesis formalization (i.e.,
Figure~\ref{fig:hypoFormOverview}) serves as a device for (i) interpretation--to
explain where and how analysts struggle in authoring statistical analyses--and
(ii) inspiration--to inspire new approaches and systems to authoring data
analyses. 

Our theory of hypothesis formalization highlights the discrepancy between
analysts' goals and the statistical software tools available to them. While
analysts want to understand their data to better understand their domains or
make decisions, the current ecosystem prioritizes mathematical expressivity and
computational control, features that are likely desirable for statistical
experts but not novices. 

As a result, designing new data analysis tools to gather conceptual knowledge
and translate them into statistical analyses is a promising approach for
statistical non-experts. In this way, hypothesis formalization retrospectively
validates our design in Tea, where its constraint-based runtime system provided
automated reasoning for Null Hypothesis Significance Tests. In order to support
more complex research questions, additional methods of explicitly grappling with
more conceptual knowledge and reasoning about different classes of statistical
analyses is necessary. We tackle this challenge for generalized linear models
with and without mixed effects in Tisane. 
% classes of statistical analyses

\textit{This work was in collaboration with Nicole de Moura, Melissa Birchfield, Jeffrey
Heer, and \reneJust. It was originally published in \tochi{2022}~\cite{} and presented
at \chiConf{2022}.}