If you are copying and pasting material from one of your papers, then remember to:
\begin{itemize}
    % \item Remove the abstract and instead add a little overview of the chapter and how it ties in to the rest of the thesis. You should also mention the original paper's source like: ``This chapter includes materials originally published in $\backslash$citet\{myownppr\}''
    % \item Make sure the formatting still works -- this is single column now!
    \item Consider rephrasing conference-paper-style language:
    \begin{itemize}
        \item Find every place you mention some variation of ``in this paper'' and say ``in this chapter'' instead.
        \item Remove or rephrase the parts where you talk about ``our main contributions''.
        \item Rephrase the language describing code and data releases.
    \end{itemize}
    % \item Replace the conclusion section with a summary section. Again, you should tie this chapter back to the main themes of the thesis.
\end{itemize}

{\color{orange} Write an overview of chapter, how it ties to the rest of the thesis.}

{\color{orange} ``This chapter includes materials originally published in $\backslash$citet\{myownppr\}''}

Although prior work has observed \textit{that} data analysis is
iterative~\cite{liu2019paths,grolemund2014cognitive} and involves multiple
levels of considerations~\cite{liu2019understanding}, \textit{how} analysts move
between these cognitive, statistical, and computational realities iteratively
has remained under-scrutinized. To fill this gap, we conducted a content analysis of 50 published
empirical publications from diverse disciplines and a lab study with 24 data
scientists authoring analyses to identify the steps and challenges involved in
authoring analyses. 

Based on our content analysis and lab study findings, we coin and define
\textit{hypothesis formalization} as a dual-search process~\cite{klahr1988dual}
that involves developing and integrating cognitive representations from two
different perspectives---conceptual hypotheses and concrete model
implementations. Analysts move back and forth between these two perspectives
during formalization while balancing conceptual, data-driven, statistical, and
programming implementation constraints. Analysts iterate over conceptual steps
to refine their hypothesis in a \textit{hypothesis refinement loop}. Analysts
also iterate over computational and implementation steps in a \textit{model
implementation loop}. Data collection and data properties may also prompt
conceptual revisions and influence statistical model implementation.

\section{Related Work: }
Our work integrates and builds upon prior research on frameworks of scientific
discovery, theories of sensemaking, statistical practices, and empirical studies
of data analysts.

\subsection{Dual-search Model of Scientific Discovery}
Klahr and Simon characterized scientific discovery as a dual-search process
involving the development and evaluation of hypotheses and
experiments~\cite{klahr1988dual}. They posited that scientific
discovery involved tasks specific to hypotheses (e.g., revising hypotheses) and
to experiments (e.g., analyzing data collected from experiments), which they
separated into two different ``spaces,'' and tasks moving between them, which is
where we place hypothesis formalization.

% exploratory data analysis would be an
% activity that impacts how analysts view, or represent, their data mentally, and
Extending Klahr and Simon's two-space model, Schunn and Klahr proposed a more
granular four-space model involving data representation, hypothesis, paradigm,
and experiment spaces~\cite{schunn1995FourSpace,schunn1996BeyondTwoSpace}. In the four-space model, conceptual hypothesizing still lies in the
hypothesis space, and hypothesis testing and statistical modeling lies in the
paradigm space. As such, hypothesis formalization is a process connecting
the hypothesis and paradigm spaces. In Schunn and Klahr's four-space model,
information flows unidirectionally from the hypothesis space to the paradigm space.
Here we extend this prior research with
evidence that hypothesis formalization involves both
concept-to-implementation and implementation-to-concept processes. (see
Figure~\ref{figure:overview}).
%  Therefore, we not only draw upon but also extend prior research on scientific discovery.
% \figureMethods 
Figure~\ref{figure:priorWork} augments Schunn and Klahr's
original diagram (Figure 1 in~\cite{schunn1995FourSpace}) with
annotations depicting how our content analysis of research papers and lab study
triangulate a tighter dual-space search between hypothesis and
paradigm spaces with a focus on hypothesis formalization. Our mixed-methods
approach follows the precedent and recommendations of Klahr and
Simon's~\cite{klahr1999studies} study of scientific discovery activities.
% using multiple methods. 

\figurePriorWorkCombined

\subsection{Theories of Sensemaking}
Human beings engage in \textit{sensemaking} to acquire new knowledge. Several
theories of
sensemaking~\cite{pirolli2005sensemaking,russell1993cost,klein2007dataFrame}
describe how and when human beings seek and integrate new data (e.g.,
observations, experiences, etc.) to develop their mental models about the world.

Russell et al.~\cite{russell1993cost} emphasize the importance of building up
and evaluating external representations of mental models, and define sensemaking
as ``the process of searching for a representation and encoding data in that
representation to answer task-specific questions.'' External representations are
critical because they influence the quality of conclusions reached at the end of
the sensemaking process and affect how much time and effort is required in the process. Some representations
may lead to insights more quickly. Russell et al. describe the iterative
process of searching for and refining external representations in a ``learning
loop complex'' that involves transitioning back and forth between (i) searching
for and (ii) instantiating representations. 
 
Grolemund and Wickham argued for statistical data analysis as a sensemaking
activity~\cite{grolemund2014cognitive}. They emphasize the (1)
bidirectional nature of updating mental models of the world and hypotheses based
on data and collecting data based on hypotheses and (2) the process of
identifying and reconciling discrepancies between hypotheses and data. Their
depiction of the analysis process parallels Klahr and Simon's framework of
scientific discovery.

% and proposed a theory of data
% analysis that includes a back and forth between an analyst's ``schema'' of how a
% phenomenon occurs in the world, a statistical model, and data. Similar to
% Russell et al., Grolemund and Wickham's model demonstrates the importance of
% representing and re-representing conceptual knowledge in schema and statistical
% models that are updated with more data. Analysts' domain expertise influence
% their schemas, which represent conceptual knowledge about known and unknown
% causal mechanisms, for example. Analysts' conceptual schema directly inform
% their hypotheses, which are statistical predictions represented in statistical
% models. These statistical models are then compared to collected data, and any
% discrepancies between the data and hypothesis require analysts to re-examine and
% possibly update their statistical model, schema, or both. 

In this paper, we consider hypothesis formalization to be a learning loop~\cite{russell1993cost} where
the conceptual hypothesis is an external representation of a set of assumptions
analysts may have about the world (e.g., an implicit causal model), that ultimately
affects which models are specified and which results are
obtained. We found that that there are smaller learning loops as analysts search
for and revise intermediate representations, such as explicit causal models,
mathematical equations, or partially specified models. The
hypothesis and model refinement loops can themselves be smaller learning loops
embedded in the larger loop of hypothesis formalization. 

% Extending Grolemund and Wickham's model, our work on
%  hypothesis formalization differentiates between conceptual and statistical
%  hypotheses and probes the phases an analyst must go through to encode a
%  conceptual hypothesis into a statistical model.

\subsection{Statistical Thinking} 
Statistical thinking and practice require differentiating between
\textit{domain} and \textit{statistical} questions. The American Statistical
Association (ASA), a professional body representing statisticians, recommends
that universities teach this fundamental principle in introductory courses (see
Goal 2 in~\cite{carver2016guidelines}). 

Similarly, researchers Wild and Pfannkuch emphasize the importance of
differentiating between and integrating statistical knowledge and context (or
domain) knowledge when thinking
statistically~\cite{pfannkuch1997statistical,pfannkuch2000statistical,wild1999statisticalThinking}.
They propose a four step model for operationalizing ideas (``inklings'') into
plans for collecting data, which are eventually statistically analyzed. In their
model, analysts must transform ``inklings'' into broad questions and then into
precise questions that are then finally turned into a plan for data collection
(see Figure 2 in~\cite{wild1999statisticalThinking}). Statistical and domain
knowledge inform all four stages. However, it is unknown what kinds of statistical and domain
knowledge are helpful, how they are used and weighed against each other, and
when certain kinds of knowledge are helpful to operationalize inklings. Our work provides more granular insight into Wild and Pfannkuch's
proposed model of operationalization and aims to answer when, how, and what
kinds of statistical and domain knowledge are used during statistical data
analysis. 

More recently, in \textit{Statistical
Rethinking}~\cite{mcelreath2020statistical}, McElreath proposes that
there are three key representational phases involved in data analysis:
conceptual hypotheses, causal models underlying hypotheses (which McElreath
calls ``process models''), and statistical models. McElreath, like the ASA and
Wild and Pfannkuch, separates domain and statistical ideas and discusses the use
of causal models as an intermediate representation to connect
the two. McElreath emphasizes that conceptual hypotheses may correspond to
multiple causal and statistical models, and that the same statistical
model may provide evidence for multiple, even contradictory, causal models and
hypotheses. McElreath's framework does not directly address how analysts navigate
these relationships or how computation plays a role, both of which we take up in
this paper. 

Overall, our work provides empirical evidence for prior frameworks but also (i)
provides more granular insight into \textit{how} and \textit{why} transitions between
representations occur and (ii) scrutinizes the role of
\textit{software and computation} through close observation of analyst workflows
in the lab as well as through a follow-up analysis of statistical software. Based on
these observations, we also speculate on how tools might better support hypothesis
formalization.
% this paper extends prior frameworks for statistical thinking by (i)
% examining the role of statistical software as analysts formalize hypotheses in a
% lab study and (ii) assessing the ways in which current software tools constrain
% hypothesis formalization. 
% \vspace{-30pt}
\subsection{Empirical Studies of Data Analysts}
Data analysis involves a number of tasks that involve data discovery, wrangling,
profiling, modeling, and reporting~\cite{kandel2012enterprise}. Extending the findings of
Kandel et al.~\cite{kandel2012enterprise}, both Alspaugh et al.~\cite{alspaugh2018futzing} and
Wongsuphasawat et al.~\cite{wongsuphasawat2019EDAgoals}
propose exploration as a distinct task.
Whereas Wongsuphasawat et al. argue that exploration should subsume
discovery and profiling, Alspaugh et al. describe exploration as an alternative
to modeling. The importance of exploration and its role in updating analysts'
understanding of the data and their goals and hypotheses is of note, regardless
of the precise order or set of tasks. Battle and Heer describe exploratory
visual analysis (EVA), a subset of exploratory data analysis (EDA) where
visualizations are the primary outputs and interfaces for exploring data, as
encompassing both data-focused (bottom-up) and goal- or hypothesis-focused
(top-down) investigations~\cite{battle2019EVA}. In our lab study, we found that
(i) analysts explored their data before modeling and (ii) exploratory
observations sometimes prompted conceptual shifts in hypotheses (bottom-up) but
at other times were guided by hypotheses and only impacted statistical
analyses (top-down). In this way, data exploration appears to be an important
intermediate step in hypothesis formalization, blurring the lines between
exploratory and confirmatory data analysis. 

Decisions throughout analysis tasks can give rise to a ``garden of forking
paths''~\cite{gelman2013garden}, which compounds for meta-analyses synthesizing
previous findings~\cite{kale2019decision}. Liu, Boukhelifa, and
Eagan~\cite{liu2019understanding} proposed a broad framework that characterizes
analysis alternatives using three different \textit{levels of abstraction}:
cognitive, artifact, and execution. \textit{Cognitive} alternatives involve more
conceptual shifts and changes (e.g., mental models, hypotheses).
\textit{Artifact} alternatives pertain to tooling (e.g., which software is used
for analysis?), model (e.g., what is the general mathematical approach?), and
data choices (e.g., which dataset is used?). \textit{Execution} alternatives are
closely related to artifact alternatives but are more fine-grained programmatic
decisions (e.g., hyperparameter tuning). We find that hypothesis formalization
involves all three levels of abstraction. We provide a more granular depiction
of how these levels cooperate with one another. 

Moreover, Liu, Althoff, and Heer~\cite{liu2019paths} identified numerous
decision points throughout the data lifecycle, which they call
\textit{end-to-end analysis}. They found that analysts often revisit key
decisions during data collection, wrangling, modeling, and evaluation. Liu,
Althoff, and Heer also found that researchers executed and selectively reported
analyses that were already found in prior work and familiar to the research
community. Hypothesis formalization is comprised of a subset of steps involved
in end-to-end analysis. Thus, we expect hypothesis formalization will be an
iterative process where domain norms will influence decision making. It is
nonetheless valuable to provide insight into how a single iteration --- from a
domain-specific research question to a single instantiation of a statistical
model (among many alternatives which may be subsequently explored) --- occurs.
Our depiction of hypothesis formalization aims to account for more
domain-general steps and artifacts, but we recognize that domain expertise and
norms may determine which paths and how quickly analysts move through hypothesis
formalization.

In summary, our work differs in (i) scope and (ii) method from prior work in HCI
on data analysis practices. Whereas hypothesis formalization has remained
implicit in prior descriptions of data analysis, we explicate
this specific process. While previous researchers have relied primarily on
post-analysis interviews with analysts, our lab study (\autoref{sec:inLabStudy}) enables us to observe
decision making during hypothesis formalization in-situ.

\input{hypothesisFormalization/content_analysis.tex}

\input{hypothesisFormalization/lab_study.tex}

\input{hypothesisFormalization/tools_analysis.tex}

\input{hypothesisFormalization/implications.tex}

\input{hypothesisFormalization/discussion.tex}

\section{Summary of Contributions}
% Replace the conclusion section with a summary section. Again, you should tie this chapter back to the main themes of the thesis.