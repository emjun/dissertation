This dissertation builds on theories of sensemaking, empirical findings on
current analytical praxis, and existing tools throughout the data lifecycle.
Additionally, this dissertation uses Donald Campbell's theory of validity to
motivate system designs and interpret evaluation results. Subsequent sections
provide additional background as applicable.

\section{Statistical data analysis as sensemaking}
Human beings engage in \textit{sensemaking} to acquire new knowledge. Several
theories of
sensemaking~\cite{pirolli2005sensemaking,russell1993cost,klein2007dataFrame}
describe how and when human beings seek and integrate new data (e.g.,
observations, experiences, etc.) to develop their mental models about the world.

Russell et al.~\cite{russell1993cost} define sensemaking as ``the process of
searching for a representation and encoding data in that representation to
answer task-specific questions.'' Russell et al. emphasize the importance of
external representations. Sensemaking is the iterative process of searching for
and refining external representations in a ``learning loop complex'' that
involves transitioning back and forth between (i) searching for and (ii)
instantiating representations. External representations are critical because
they influence the quality of conclusions reached at the end of the sensemaking
process and affect how much time and effort is required in the process. Some
representations may lead to insights more quickly. Indeed, we posit and find
that statistical analysis, specifically hypothesis formalization
(\autoref{chapter:hypoForm}), is a learning loop~\cite{russell1993cost} where
the conceptual research question or hypothesis is an external representation of
a set of assumptions analysts may have about the world (e.g., an implicit causal
model), that ultimately affects which statistical models are implemented and
which results are obtained. We also find that there are smaller learning
loops---for revising explicit causal models, mathematical equations, and
partially specified models---embedded in the larger loop of hypothesis
formalization.

Grolemund and Wickham argued for statistical data analysis as a sensemaking
activity~\cite{grolemund2014cognitive}. They emphasize the (1) bidirectional
nature of updating mental models of the world and hypotheses based on data and
collecting data based on hypotheses and (2) the process of identifying and
reconciling discrepancies between hypotheses and data. Similar to Russell et
al., Grolemund and Wickham's model demonstrates the importance of representing
and re-representing conceptual knowledge. Grolemund and Wickham's theory of data
analysis includes a back and forth between an analyst's ``schema'' of how a
phenomenon occurs in the world, a statistical model, and data. Analysts' domain
expertise influence their schemas, which represent conceptual knowledge about
known and unknown causal mechanisms, for example. Analysts' conceptual schema
directly inform their hypotheses, which are statistical predictions represented
in statistical models. These statistical models are then compared to collected
data, and any discrepancies between the data and hypothesis require analysts to
re-examine and possibly update their statistical model, schema, or both.
Extending Grolemund and Wickham's model, our work on hypothesis formalization
differentiates between conceptual and statistical hypotheses and probes the
phases an analyst must go through to encode a conceptual hypothesis into a
statistical model.

Given the centrality of external representations of implicit conceptual
knowledge to authoring statistical analyses that help analysts make sense of
the world, we argue that our statistical software should focus on helping
analysts to express their conceptual hypotheses and implicit domain knowledge.
Through the development of two software systems, \tea (~\autoref{chapter:tea})
and Tisane(~\autoref{chapter:tisane}), we explore \textit{how} to design
programming abstractions and \textit{what} those abstractions should include in
order for statistical non-experts to externalize their implicit conceptual
knowledge about a domain. 

\section{Empirical accounts of data analysis practice}
% In support of this theoretical view of data analysis as sensemaking, studies
% with analysts have found that data analysis is an iterative process that
% involves data collection; cleaning and wrangling; and statistical testing and
% modeling~\cite{kandel2012enterprise,alspaugh2018futzing,wongsuphasawat2019EDAgoals,grolemund2014cognitive,liu2019paths,liu2019understanding}.
% The importance of exploration and its role in updating analysts' understanding
% of the data and their goals and hypotheses is of note. 

Data analysis involves a number of tasks that involve data discovery, wrangling,
profiling, modeling, and reporting~\cite{kandel2012enterprise}. Extending the
findings of Kandel et al.~\cite{kandel2012enterprise}, both Alspaugh et
al.~\cite{alspaugh2018futzing} and Wongsuphasawat et
al.~\cite{wongsuphasawat2019EDAgoals} propose exploration as a distinct task.
Whereas Wongsuphasawat et al. argue that exploration should subsume discovery
and profiling, Alspaugh et al. describe exploration as an alternative
to modeling. The importance of exploration and its role in updating analysts'
understanding of the data and their goals and hypotheses is of note, regardless
of the precise order or set of tasks. Battle and Heer describe
exploratory visual analysis (EVA), a subset of exploratory data analysis (EDA)
where visualizations are the primary interfaces and outputs for exploring data,
as encompassing both data-focused (bottom-up) and goal- or hypothesis-focused
(top-down) investigations~\cite{battle2019EVA}. In~\autoref{chapter:hypoForm},
we found that (i) analysts explored their data before modeling and (ii)
exploratory observations sometimes prompted conceptual shifts in hypotheses
(bottom-up) but at other times were guided by hypotheses and only impacted
statistical analyses (top-down). In this way, data exploration appears to be an
important intermediate step in hypothesis formalization, blurring the lines
between exploratory and confirmatory data analysis. 

Decisions throughout analysis tasks can give rise to a ``garden of forking
paths''~\cite{gelman2013garden}, which compounds for meta-analyses synthesizing
previous findings~\cite{kale2019decision}. Liu, Boukhelifa, and
Eagan~\cite{liu2019understanding} proposed a broad framework that characterizes
analysis alternatives using three different \textit{levels of abstraction}:
cognitive (e.g., shifts in conceptual hypotheses), artifact (e.g., choice in
statistical tools), and execution (e.g., computational tuning).
\textit{Cognitive} alternatives involve more conceptual shifts and changes
(e.g., mental models, hypotheses). \textit{Artifact} alternatives pertain to
tooling (e.g., which software is used for analysis?), model (e.g., what is the
general mathematical approach?), and data choices (e.g., which dataset is
used?). \textit{Execution} alternatives are closely related to artifact
alternatives but are more fine-grained programmatic decisions (e.g.,
hyperparameter tuning). We find that hypothesis formalization involves all three
levels of abstraction and provide a more granular depiction of how these levels
cooperate with one another (\autoref{chapter:hypoForm}). 

Moreover, Liu, Althoff, and Heer~\cite{liu2019paths} identified numerous
decision points throughout the data lifecycle, which they call
\textit{end-to-end analysis}. They found that analysts often revisit key
decisions during data collection, wrangling, modeling, and evaluation. Liu,
Althoff, and Heer also found that researchers executed and selectively reported
analyses that were already found in prior work and familiar to the research
community. The focus of this thesis is on how any single pass or iteration
occurs. We approach this work from the perspective that by understanding a
single iteration, we may be able to focus analysts on their iterations that are
most substantial and impactful and eliminate a number of unnecessary iterations
that arise due to mistakes in aligning conceptual and statistical concerns,
which we found in our case studies (see ~\autoref{sec:tisane_case_studies}).

% Our depiction of hypothesis formalization aims to account for more
% domain-general steps and artifacts, but we recognize that domain expertise and
% norms may determine which paths and how quickly analysts move through hypothesis
% formalization.

Importantly, our work differs in (i) scope and (ii) method from prior work in
HCI on data analysis practices. Whereas translating a research question or
hypothesis into a statistical analysis has remained implicit in prior
descriptions of data analysis, we explicate this specific process. Additionally,
while previous researchers have relied primarily on post-analysis interviews
with analysts, our lab study (\autoref{sec:inLabStudy}) enables us to observe
decision making during this process in-situ.


% Whereas hypothesis formalization
% has remained implicit in prior descriptions of data analysis, we explicate this
% specific process. Furthermore, our work differs in method. While previous
% researchers have relied primarily on post-analysis interviews with analysts, our
% lab study (\autoref{sec:inLabStudy}) enables us to observe decision making
% during hypothesis formalization in-situ. In summary, our work differs in (i)
% scope and (ii) method from prior work in HCI on data analysis practices. 

\section{Tools for data analysis}
The software ecosystem for data analysis is vibrant, with numerous programming
languages, software packages, and graphical-first tools. A common limitation of
existing software is its siloing of statistical specification from the
conceptual and data collection details that inadvertently influence statistical
analysis. In contrast, the systems in this dissertation explore ways to leverage
implicit conceptual and data collection knowledge to derive statistical
analyses. Below, we compare and contrast this dissertation with existing
software for conceptual modeling, study design, and statistical specification. 

% In this dissertation, we explore the levels of abstraction we can
% provide to meet users where they are comfortable (and their goals) while also
% having sufficient information to reason about statistical methods. We also explore
% two different reasoning methods (constraint-based in \tea and graph-based in
% Tisane) and find that a hybrid approach (\teasane) may be the most promising for realizing
% the vision of supporting hypothesis formalization end-to-end.

\subsection{Tools for conceptual modeling}
For statistical experts, causal diagramming is a common approach to
externalizing implicit concpetual models. For instance,
Daggity~\cite{textor2011dagitty} supports authoring, editing, and formally
analyzing causal graphs through code and a visual editor. The key limitation of
Daggity is that it requires analysts to specify a formal causal graph, which
statistical non-experts, including many domain experts, may not be able to
do~\cite{suzuki2020causal,suzuki2018mechanisms,velentgas2013developing}. In
fact, an open challenge for causal reasoning and discovery is in getting domain
experts to express their implicit knowledge in a way that can be formally
represented and reasoned about. Our work on \tisane directly addresses this
challenge. Moreover, even if analysts are able to express causal diagrams in
Dagitty, Dagitty does not translate queries analysts may have about the causal
diagram (i.e., research questions, hypotheses) into statistical models that
could assess specific relationships of interest. \tisane also overcomes this
limitation for a set of queries about average causal effect. 
% Although a knowledgeable analyst could use Daggity to identify a set of
% variables that control for confounding to include in a linear model
% (``adjustment sets''), Daggity does not support translation of these statistical
% insights into model implementation code. 

\polish{For experts comfortable working with causal diagrams, software packages such as marginaleffects}

\def\edibble{\texttt{edibble}\xspace}
\subsection{Tools for study design}
Several domain-specific languages~\cite{gosset,bakshy2014planout}, software
packages~\cite{edibble,blair2019declaring}, and standalone
applications~\cite{mackay2007touchstone,eiselmayer2019touchstone2} specialize in
experiment design. A primary focus is to provide researchers low-level control
over trial-level and randomization details. For example,
JsPsych~\cite{deLeeuw2015jspsych} gives researchers fine-grained control over
the design and presentation of stimuli for online experiments. At a mid-level of
abstraction, Touchstone~\cite{mackay2007touchstone} is a %comprehensive
tool for designing and launching online experiments. It also refers users to R
and JMP for data analysis but does not help users author an appropriate
statistical model. Touchstone2~\cite{eiselmayer2019touchstone2} helps
researchers design experiments based on statistical power. At a high-level of
abstraction, \edibble~\cite{edibble} helps researchers plan their data collection
schema. \edibble aims to provide a ``grammar of study design'' that focuses users
on their experimental manipulations in relation to specific units (e.g.,
participants, students, schools), the frequency and distribution of conditions
(e.g., within-subjects vs. between-subjects), and measures to collect (e.g.,
age, grade, location) in order to output a table to fill in during data
collection. While \tisane's \SDSLlong uses an abstraction level comparable to
\edibble, \tisane is focused on using the expressed data measurement relationships
to infer a statistical model. Additionally, \tisane's \SDSL provides conceptual
relationships that are out of the scope of \edibble but important for specifying
conceptually valid statistical models.

% Touchstone~\cite{mackay2007touchstone} is a comprehensive tool that
% supports the design and launch of online experiments. Touchstone provides
% suggestions for data analysis based on experimental design.
% Touchstone2~\cite{eiselmayer2019touchstone2} builds upon Touchstone and provides
% more extensive guidance for evaluating the impact of experimental design on
% statistical power. 

\subsection{Tools for statistical specification}
A contribution of this thesis is a closer examination of how existing
statistical analysis tools fail to support authoring
(\autoref{sec:toolsAnalysis}). Here, we contrast the systems developed in this
thesis to discipline-specific software tools for research and more general
automated statistics approaches. 

Research has introduced tools to support statistical analysis in diverse
domains. ExperiScope~\cite{guimbretiere2007experiscope} supports users in
analyzing complex data logs for interaction techniques. ExperiScope surfaces
patterns in the data that would be difficult to detect manually and enables
researchers to collect noisier data in the wild that have greater external
validity. Statsplorer~\cite{wacharamanotham2015statsplorer} is an educational
web application for novices learning about statistics. While more focused on
visualizing various alternatives for statistical tests, Statsplorer automates
test selection (for a limited number of statistical tests and by executing
simple switch statements) and the checking of assumptions (though it is
currently limited to tests of normality and equal
variance).~\cite{wacharamanotham2015statsplorer} found that Statsplorer helps
HCI students perform better in a subsequent statistics lecture. Similar in scope
to Statsplorer, \tea is designed to help statistical non-experts author
Null-Hypothesis Significance Tests. \tea supports twice as many statistical
tests as Statsplorer, suggesting that \tea's constraint-based approach is more
expressive than Statsplorer's decision-tree implementation for statistical test
selection. In contrast to the above systems, a key design consideration for \tea
and \tisane has been their ability to apply widely across disciplines and
integrate into many existing workflows. Therefore, the systems are implemented
as embedded DSLs in Python and R, two widely used programming languages for data
science.

The Automatic Statistician~\cite{lloyd2014automatic} generates a report listing
all ``interesting'' relationships (e.g., correlations, statistical models,
etc.). %among data columns.
Although apparently complete, the Automatic Statistician may overlook analyses
that are conceptually interesting and difficult, if not impossible, to deduce
from data alone. Furthermore, AutoML tools such as Auto-WEKA~\cite{autoweka},
auto-sklearn~\cite{autosklearn}, and H2O AutoML~\cite{H2OAutoML20} also
prioritize finding patterns in data and aim to make statistical methods more
widely usable. However, \tea and \tisane differ from AutoML efforts in their
researchers developing scientific theories. As a result, \tisane provides focus
on analysts who prioritize explanation, not just prediction, such as support for
specifying GLMMs, which some prominent AutoML tools, such as
auto-sklearn~\cite{autosklearn}, omit. 

%However, Statsplorer is an educational web application that supports a limited number of statistical tests and grounds the analysis procedure in data visualizations. 

\section{Validity in statistical data analysis}
Finally, a aspect of this thesis is that software with conceptually grounded
programming abstractions and automated reasoning can improve the validity of
analyses. There are many working definitions of ``validity,'' from predictive
accuracy to a quality of how well experiments are designed to a trade-off
between model simplicity and fit (e.g., R-squared). Donald Campbell's theory of
validity~\cite{shadish2010campbell}, widely adopted across disciplines, provides
a framework for reasoning about and unifying many intuitive definitions of
validity. Campbell defines four dimensions of validity: internal validity,
external validity, statistical conclusion validity, and construct validity. This
thesis focuses on enhancing statistical conclusion, external, and internal
validity through the correct application and specification of statistical
analyses that match analysts' intentions (i.e., their research questions and
hypotheses) and data collection procedures. We do not address construct validity
because construct validity is specific to a discipline's theories and is often
debated over a relatively long period of time. \polish{In the conclusion
(\autoref{chapter:conclusion}), we pick back up on opportunities to address
construct validity through the application of recent natural language processing
advances.}

% all of which impact how statistical analyses are
% conducted and interpreted. \todo{Table for each | definition | mistakes/threats
% that our work addresses}. 

% \ej{Which dimensions of statistical validity do we care about?
% - Why is statistical test selection and model authoring important? -- not the
% only important things, which is why I want to explore ways to more directly
% guide data collection in \teasane}