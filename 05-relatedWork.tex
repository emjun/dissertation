This work builds on theories of scientific discovery and sensemaking, empirical
findings on current analytical praxis, and existing tools in the data lifecycle.
Subsequent sections provide additional statistical background as needed.

\section{Theories of Scientific Discovery and Sensemaking}
Klahr and Simon characterized scientific discovery as a dual-search process
involving the development and evaluation of hypotheses and
experiments~\cite{klahr1988dual}. They posited that scientific discovery
involved tasks specific to hypotheses (e.g., revising hypotheses) and to
experiments (e.g., analyzing data collected from experiments), which they
separated into two different ``spaces,'' and tasks moving between them, which is
where we place hypothesis formalization. Extending Klahr and Simon's two-space
model, Schunn and Klahr proposed a more granular four-space model involving data
representation, hypothesis, paradigm, and experiment
spaces~\cite{schunn1995FourSpace,schunn1996BeyondTwoSpace}. In the four-space
model, conceptual hypothesizing still lies in the hypothesis space, and
hypothesis testing and statistical modeling lies in the paradigm space. As such,
hypothesis formalization is a process connecting the hypothesis and paradigm
spaces. In Schunn and Klahr's four-space model, information flows
unidirectionally from the hypothesis space to the paradigm space.
In~\autoref{sec:hypothesisFormalization} we extend this prior research with
evidence that the path from hypothesis and paradigm spaces is actually
bidirectional. (see Figure~\ref{figure:overview}).

% Figure~\ref{figure:priorWork} augments Schunn and Klahr's
% original diagram (Figure 1 in~\cite{schunn1995FourSpace}) with
% annotations depicting how our content analysis of research papers and lab study
% triangulate a tighter dual-space search between hypothesis and
% paradigm spaces with a focus on hypothesis formalization. Our mixed-methods
% approach follows the precedent and recommendations of Klahr and
% Simon's~\cite{klahr1999studies} study of scientific discovery activities.
% using multiple methods. 
% \figurePriorWorkCombined

% Similar to Klahr and Simon's framework of scientific discovery, 
Grolemund and Wickham describe the analysis process as (i) updating mental
models of the world and hypotheses based on data, (ii) collecting data based on
hypotheses, and (iii) identifying and reconciling discrepancies between
hypotheses and data\cite{grolemund2014cognitive}. As such, Grolemund and
Wickham argue for statistical data analysis as a sensemaking activity. 

% Rusell et al. emphasize the
% importance of external representations during sensemaking because they can
% influence the quality of conclusions reached and the time to arrive at them. 
Russell et al.~\cite{russell1993cost} define sensemaking as ``the process of
searching for a representation and encoding data in that representation to
answer task-specific questions.'' Sensemaking is the iterative process of
searching for and refining external representations in a ``learning loop
complex'' that involves transitioning back and forth between (i) searching for
and (ii) instantiating representations.
% , which in the context of data analysis
% are conceptual hypotheses and statistical model implementations. 
Indeed, we find that hypothesis formalization is a learning
loop~\cite{russell1993cost} where the conceptual hypothesis is an external
representation of a set of assumptions analysts may have about the world (e.g.,
an implicit causal model), that ultimately affects which statistical models are
implemented and which results are obtained. 

Klahr and Simon characterized scientific discovery as a dual-search process
involving the development and evaluation of hypotheses and
experiments~\cite{klahr1988dual}. They posited that scientific
discovery involved tasks specific to hypotheses (e.g., revising hypotheses) and
to experiments (e.g., analyzing data collected from experiments), which they
separated into two different ``spaces,'' and tasks moving between them, which is
where we place hypothesis formalization.

% exploratory data analysis would be an
% activity that impacts how analysts view, or represent, their data mentally, and
Extending Klahr and Simon's two-space model, Schunn and Klahr proposed a more
granular four-space model involving data representation, hypothesis, paradigm,
and experiment spaces~\cite{schunn1995FourSpace,schunn1996BeyondTwoSpace}. In the four-space model, conceptual hypothesizing still lies in the
hypothesis space, and hypothesis testing and statistical modeling lies in the
paradigm space. As such, hypothesis formalization is a process connecting
the hypothesis and paradigm spaces. In Schunn and Klahr's four-space model,
information flows unidirectionally from the hypothesis space to the paradigm space.
Here we extend this prior research with
evidence that hypothesis formalization involves both
concept-to-implementation and implementation-to-concept processes. (see
Figure~\ref{figure:overview}).
%  Therefore, we not only draw upon but also extend prior research on scientific discovery.
% \figureMethods 
% Figure~\ref{figure:priorWork} augments Schunn and Klahr's
% original diagram (Figure 1 in~\cite{schunn1995FourSpace}) with
% annotations depicting how our content analysis of research papers and lab study
% triangulate a tighter dual-space search between hypothesis and
% paradigm spaces with a focus on hypothesis formalization. Our mixed-methods
% approach follows the precedent and recommendations of Klahr and
% Simon's~\cite{klahr1999studies} study of scientific discovery activities.
% using multiple methods. 

\figurePriorWorkCombined

\subsection{Theories of Sensemaking}
Human beings engage in \textit{sensemaking} to acquire new knowledge. Several
theories of
sensemaking~\cite{pirolli2005sensemaking,russell1993cost,klein2007dataFrame}
describe how and when human beings seek and integrate new data (e.g.,
observations, experiences, etc.) to develop their mental models about the world.

Russell et al.~\cite{russell1993cost} emphasize the importance of building up
and evaluating external representations of mental models, and define sensemaking
as ``the process of searching for a representation and encoding data in that
representation to answer task-specific questions.'' External representations are
critical because they influence the quality of conclusions reached at the end of
the sensemaking process and affect how much time and effort is required in the process. Some representations
may lead to insights more quickly. Russell et al. describe the iterative
process of searching for and refining external representations in a ``learning
loop complex'' that involves transitioning back and forth between (i) searching
for and (ii) instantiating representations. 
 
Grolemund and Wickham argued for statistical data analysis as a sensemaking
activity~\cite{grolemund2014cognitive}. They emphasize the (1)
bidirectional nature of updating mental models of the world and hypotheses based
on data and collecting data based on hypotheses and (2) the process of
identifying and reconciling discrepancies between hypotheses and data. Their
depiction of the analysis process parallels Klahr and Simon's framework of
scientific discovery.

and proposed a theory of data
analysis that includes a back and forth between an analyst's ``schema'' of how a
phenomenon occurs in the world, a statistical model, and data. Similar to
Russell et al., Grolemund and Wickham's model demonstrates the importance of
representing and re-representing conceptual knowledge in schema and statistical
models that are updated with more data. Analysts' domain expertise influence
their schemas, which represent conceptual knowledge about known and unknown
causal mechanisms, for example. Analysts' conceptual schema directly inform
their hypotheses, which are statistical predictions represented in statistical
models. These statistical models are then compared to collected data, and any
discrepancies between the data and hypothesis require analysts to re-examine and
possibly update their statistical model, schema, or both. 

In this paper, we consider hypothesis formalization to be a learning loop~\cite{russell1993cost} where
the conceptual hypothesis is an external representation of a set of assumptions
analysts may have about the world (e.g., an implicit causal model), that ultimately
affects which models are specified and which results are
obtained. We found that that there are smaller learning loops as analysts search
for and revise intermediate representations, such as explicit causal models,
mathematical equations, or partially specified models. The
hypothesis and model refinement loops can themselves be smaller learning loops
embedded in the larger loop of hypothesis formalization. 

Extending Grolemund and Wickham's model, our work on
 hypothesis formalization differentiates between conceptual and statistical
 hypotheses and probes the phases an analyst must go through to encode a
 conceptual hypothesis into a statistical model.


\section{Empirical accounts of data analysis practice}
Studies with analysts have found that data analysis is an iterative process that
involves data collection; cleaning and wrangling; and statistical testing and
modeling~\cite{kandel2012enterprise,alspaugh2018futzing,wongsuphasawat2019EDAgoals,grolemund2014cognitive,liu2019paths,liu2019understanding}.
The importance of exploration and its role in updating analysts' understanding
of the data and their goals and hypotheses is of note. Battle and Heer describe
exploratory visual analysis (EVA), a subset of exploratory data analysis (EDA)
where visualizations are the primary interfaces and outputs for exploring data,
as encompassing both data-focused (bottom-up) and goal- or hypothesis-focused
(top-down) investigations~\cite{battle2019EVA}.
In~\autoref{sec:hypothesisFormalization}, we found that (i) analysts explored
their data before modeling and (ii) exploratory observations sometimes prompted
conceptual shifts in hypotheses (bottom-up) but at other times were guided by
hypotheses and only impacted statistical analyses (top-down). In this way, data
exploration appears to be an important intermediate step in hypothesis
formalization, blurring the lines between exploratory and confirmatory data
analysis. 

Decisions throughout analysis tasks can give rise to a ``garden of forking
paths''~\cite{gelman2013garden}, which compounds for meta-analyses synthesizing
previous findings~\cite{kale2019decision}. Liu, Boukhelifa, and
Eagan~\cite{liu2019understanding} proposed a broad framework that characterizes
analysis alternatives using three different \textit{levels of abstraction}:
cognitive (e.g., shifts in conceptual hypotheses), artifact (e.g., choice in
statistical tools), and execution (e.g., computational tuning). Liu, Althoff,
and Heer~\cite{liu2019paths} found that analysts often revisit key decisions
during data collection, wrangling, modeling, and evaluation. The focus of this
thesis is on how any single pass or iteration occurs. I approach this work from
the perspective that by understanding a single iteration, we may be able to
focus analysts on their iterations that are most substantial and impactful and
eliminate a number of unnecessary iterations that arise due to mistakes in
aligning conceptual and statistical concerns, which we found in our case studies
(see ~\autoref{sec:tisane_case_studies}).

\subsection{Empirical Studies of Data Analysts}
Data analysis involves a number of tasks that involve data discovery, wrangling,
profiling, modeling, and reporting~\cite{kandel2012enterprise}. Extending the findings of
Kandel et al.~\cite{kandel2012enterprise}, both Alspaugh et al.~\cite{alspaugh2018futzing} and
Wongsuphasawat et al.~\cite{wongsuphasawat2019EDAgoals}
propose exploration as a distinct task.
Whereas Wongsuphasawat et al. argue that exploration should subsume
discovery and profiling, Alspaugh et al. describe exploration as an alternative
to modeling. The importance of exploration and its role in updating analysts'
understanding of the data and their goals and hypotheses is of note, regardless
of the precise order or set of tasks. Battle and Heer describe exploratory
visual analysis (EVA), a subset of exploratory data analysis (EDA) where
visualizations are the primary outputs and interfaces for exploring data, as
encompassing both data-focused (bottom-up) and goal- or hypothesis-focused
(top-down) investigations~\cite{battle2019EVA}. In our lab study, we found that
(i) analysts explored their data before modeling and (ii) exploratory
observations sometimes prompted conceptual shifts in hypotheses (bottom-up) but
at other times were guided by hypotheses and only impacted statistical
analyses (top-down). In this way, data exploration appears to be an important
intermediate step in hypothesis formalization, blurring the lines between
exploratory and confirmatory data analysis. 

Decisions throughout analysis tasks can give rise to a ``garden of forking
paths''~\cite{gelman2013garden}, which compounds for meta-analyses synthesizing
previous findings~\cite{kale2019decision}. Liu, Boukhelifa, and
Eagan~\cite{liu2019understanding} proposed a broad framework that characterizes
analysis alternatives using three different \textit{levels of abstraction}:
cognitive, artifact, and execution. \textit{Cognitive} alternatives involve more
conceptual shifts and changes (e.g., mental models, hypotheses).
\textit{Artifact} alternatives pertain to tooling (e.g., which software is used
for analysis?), model (e.g., what is the general mathematical approach?), and
data choices (e.g., which dataset is used?). \textit{Execution} alternatives are
closely related to artifact alternatives but are more fine-grained programmatic
decisions (e.g., hyperparameter tuning). We find that hypothesis formalization
involves all three levels of abstraction. We provide a more granular depiction
of how these levels cooperate with one another. 

Moreover, Liu, Althoff, and Heer~\cite{liu2019paths} identified numerous
decision points throughout the data lifecycle, which they call
\textit{end-to-end analysis}. They found that analysts often revisit key
decisions during data collection, wrangling, modeling, and evaluation. Liu,
Althoff, and Heer also found that researchers executed and selectively reported
analyses that were already found in prior work and familiar to the research
community. Hypothesis formalization is comprised of a subset of steps involved
in end-to-end analysis. Thus, we expect hypothesis formalization will be an
iterative process where domain norms will influence decision making. It is
nonetheless valuable to provide insight into how a single iteration --- from a
domain-specific research question to a single instantiation of a statistical
model (among many alternatives which may be subsequently explored) --- occurs.
Our depiction of hypothesis formalization aims to account for more
domain-general steps and artifacts, but we recognize that domain expertise and
norms may determine which paths and how quickly analysts move through hypothesis
formalization.

In summary, our work differs in (i) scope and (ii) method from prior work in HCI
on data analysis practices. Whereas hypothesis formalization has remained
implicit in prior descriptions of data analysis, we explicate
this specific process. While previous researchers have relied primarily on
post-analysis interviews with analysts, our lab study (\autoref{sec:inLabStudy}) enables us to observe
decision making during hypothesis formalization in-situ.


% Whereas hypothesis formalization
% has remained implicit in prior descriptions of data analysis, we explicate this
% specific process. Furthermore, our work differs in method. While previous
% researchers have relied primarily on post-analysis interviews with analysts, our
% lab study (\autoref{sec:inLabStudy}) enables us to observe decision making
% during hypothesis formalization in-situ. In summary, our work differs in (i)
% scope and (ii) method from prior work in HCI on data analysis practices. 



\section{Tools for data analysis}
% \ej{Key takeaway: Tools don't focus on hypothesis formalization.}
% Most existing tools focus on one set of concerns at a time. For example,
% conceptual modeling...
% study design...
% statistical analysis...(what resonated with Tyler in the Tisane paper)

% Whereas hypothesis formalization
% has remained implicit in prior descriptions of data analysis, we explicate this
% specific process. 

There are numerous software tools for data analysis. Most focus on isolating one
set of concerns--conceptual modeling, study design, or statistical
specification. Although a singular focus is necessary for developing effective
and modular software, integrating across these concerns is necessary for
accurate hypothesis formalization. This integration is underrepresented in the
current ecosystem of tools and is the focus of the systems built in this
dissertation.

% In this dissertation, we explore the levels of abstraction we can
% provide to meet users where they are comfortable (and their goals) while also
% having sufficient information to reason about statistical methods. We also explore
% two different reasoning methods (constraint-based in Tea and graph-based in
% Tisane) and find that a hybrid approach (Teasane) may be the most promising for realizing
% the vision of supporting hypothesis formalization end-to-end.

\subsection{Tools for conceptual modeling}
Daggity~\cite{textor2011dagitty} supports authoring, editing, and
formally analyzing causal graphs through code and a visual editor. Daggity
requires users to specify a formal causal graph, which may not always be
possible~\cite{suzuki2020causal,suzuki2018mechanisms,velentgas2013developing}.
Although a knowledgeable analyst could use Daggity to identify a set of
variables that control for confounding to include in a linear model
(``adjustment sets''), Daggity does not support translation of these statistical
insights into model implementation code. 

\subsection{Tools for study design}
Several domain-specific languages~\cite{gosset,bakshy2014planout}, software
packages~\cite{edibble,blair2019declaring}, and standalone
applications~\cite{mackay2007touchstone,eiselmayer2019touchstone2} specialize in
experiment design. A primary focus is to provide researchers low-level control
over trial-level and randomization details. For example,
JsPsych~\cite{deLeeuw2015jspsych} gives researchers fine-grained control over
the design and presentation of stimuli for online experiments. At a mid-level of
abstraction, Touchstone~\cite{mackay2007touchstone} is a %comprehensive
tool for designing and launching online experiments. It also refers users to R
and JMP for data analysis but does not help users author an appropriate
statistical model. Touchstone2~\cite{eiselmayer2019touchstone2} helps
researchers design experiments based on statistical power. At a high-level of
abstraction, edibble~\cite{edibble} helps researchers plan their data collection
schema. Edibble aims to provide a ``grammar of study design'' that focuses users
on their experimental manipulations in relation to specific units (e.g.,
participants, students, schools), the frequency and distribution of conditions
(e.g., within-subjects vs. between-subjects), and measures to collect (e.g.,
age, grade, location) in order to output a table to fill in during data
collection. While Tisane's \SDSLlong uses an abstraction level comparable to
edibble, Tisane is focused on using the expressed data measurement relationships
to infer a statistical model. Additionally, Tisane's \SDSL provides conceptual
relationships that are out of the scope of edibble but important for specifying
conceptually valid statistical models.

\subsection{Tools for statistical specification}
\todo{Forward reference ~\autoref{sec:toolsAnalysis}}
AutoML tools such as
Auto-WEKA~\cite{autoweka}, auto-sklearn~\cite{autosklearn}, and H2O
AutoML~\cite{H2OAutoML20} aim to make statistical methods more widely usable.
Tea and Tisane differ from AutoML efforts in their focus on analysts who
prioritize explanation, not just prediction, such as researchers developing
scientific theories. As a result, Tisane provides support for specifying GLMMs,
which some prominent AutoML tools, such as auto-sklearn~\cite{autosklearn},
omit. 

The Automatic Statistician~\cite{lloyd2014automatic} generates a report listing
all ``interesting'' relationships (e.g., correlations, statistical models,
etc.). %among data columns.
Although apparently complete, the Automatic Statistician may overlook analyses
that are conceptually interesting and difficult, if not impossible, to deduce
from data alone.

Recent work in the database community helps researchers answer causal questions
about multilevel, or hierarchical, data~\cite{salimi2020causal, kayali2020demonstration}.
% \footnote{Generalized linear models with mixed effects are appropriate in these settings as well.},
CaRL~\cite{salimi2020causal} provides a domain-specific language to express
causal relationships between variables and a GUI to show researchers %the
results. Tea and Tisane leverage a similar insight that researchers have domain
knowledge that a system can use to infer statistical methods. Whereas CaRL is
focused on answering specific queries about average causal effect, the systems
in this dissertation are designed to address a range of non-causal questions.

% \vspace{-13pt}
\subsubsection{Tools for Statistical Analysis}
Research has also introduced tools to support statistical analysis in diverse
domains. ExperiScope~\cite{guimbretiere2007experiscope} supports users in
analyzing complex data logs for interaction techniques. ExperiScope surfaces
patterns in the data that would be difficult to detect manually and enables
researchers to collect noisier data in the wild that have greater external
validity. Touchstone~\cite{mackay2007touchstone} is a comprehensive tool that
supports the design and launch of online experiments. Touchstone provides
suggestions for data analysis based on experimental design.
Touchstone2~\cite{eiselmayer2019touchstone2} builds upon Touchstone and provides
more extensive guidance for evaluating the impact of experimental design on
statistical power. Statsplorer~\cite{wacharamanotham2015statsplorer} is an
educational web application for novices learning about statistics. While more
focused on visualizing various alternatives for statistical tests, Statsplorer
also automates test selection (for a limited number of statistical tests and by
executing simple switch statements) and the checking of assumptions (though it
is currently limited to tests of normality and equal
variance).~\cite{wacharamanotham2015statsplorer} found that Statsplorer helps
HCI students perform better in a subsequent statistics lecture. 

In comparison to Statsplorer, Tea is specifically designed to integrate into
existing workflows. Tea can be executed in any Python environment, including
notebooks, which are widely used in data analysis. Tea enables reproducing and
extending analyses by being script-based, and the analyses are focused on
hypotheses that analysts specify. 

%However, Statsplorer is an educational web application that supports a limited number of statistical tests and grounds the analysis procedure in data visualizations. 


\section{Validity in statistical data analysis}
Finally, there are many working definitions of ``validity,'' from predictive
accuracy to a quality of how well experiments are designed to a trade-off
between model simplicity and fit (e.g., R-squared). Donald Campbell's theory of
validity~\cite{shadish2010campbell} provides a framework for reasoning about and
unifying many intuitive definitions of validity. The Campbellian theory of
validity has also become widely adopted across
disciplines~\cite{shadish2010campbell}. Campbell defines four dimensions of
validity: internal validity, external validity, statistical conclusion validity,
and construct validity. This thesis focuses on enhancing statistical conclusion,
external, and internal validity through the correct application and
specification of statistical analyses that match end-user intentions and data
collection procedures. We do not address construct validity because construct
validity tends to be more domain-specific and theoretical as well as a quality
assessed about a measure over a period of time. 

% all of which impact how statistical analyses are
% conducted and interpreted. \todo{Table for each | definition | mistakes/threats
% that our work addresses}. 

% \ej{Which dimensions of statistical validity do we care about?
% - Why is statistical test selection and model authoring important? -- not the
% only important things, which is why I want to explore ways to more directly
% guide data collection in Teasane}

